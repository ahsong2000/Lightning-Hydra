{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42d0273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9cfaeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02bbf4e",
   "metadata": {},
   "source": [
    "### Step 1: Define LightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc1ac2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitAutoEncoder(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, batch_size=10):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28*28, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 3)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 28*28)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # in lightning, forward defines the prediction/inference actions\n",
    "        embedding = self.encoder(x)\n",
    "        return embedding\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defined the train loop.\n",
    "        # It is independent of forward\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "        # Logging to TensorBoard by default\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6470d9",
   "metadata": {},
   "source": [
    "### Step 2: Fit with Lightning Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d2309dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /home/ahsong/Shell_DS/91_lightning-hydra/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 503: Service Unavailable\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /home/ahsong/Shell_DS/91_lightning-hydra/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bde9e6b56e04ebfae243cc4a21253d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/ahsong/Shell_DS/91_lightning-hydra/MNIST/raw/train-images-idx3-ubyte.gz to /home/ahsong/Shell_DS/91_lightning-hydra/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 503: Service Unavailable\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /home/ahsong/Shell_DS/91_lightning-hydra/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e3619068e4942a199d65bbe56ea85b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/ahsong/Shell_DS/91_lightning-hydra/MNIST/raw/train-labels-idx1-ubyte.gz to /home/ahsong/Shell_DS/91_lightning-hydra/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 503: Service Unavailable\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /home/ahsong/Shell_DS/91_lightning-hydra/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e427e38b71a7451183ef324ff89768e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/ahsong/Shell_DS/91_lightning-hydra/MNIST/raw/t10k-images-idx3-ubyte.gz to /home/ahsong/Shell_DS/91_lightning-hydra/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /home/ahsong/Shell_DS/91_lightning-hydra/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c84870ca25d5476596ecafabebbdad64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/ahsong/Shell_DS/91_lightning-hydra/MNIST/raw/t10k-labels-idx1-ubyte.gz to /home/ahsong/Shell_DS/91_lightning-hydra/MNIST/raw\n",
      "\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahsong/anaconda3/envs/ray_aws/lib/python3.8/site-packages/torchvision/datasets/mnist.py:502: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1616554788289/work/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "dataset = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "817e9d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | encoder | Sequential | 50.4 K\n",
      "1 | decoder | Sequential | 51.2 K\n",
      "---------------------------------------\n",
      "101 K     Trainable params\n",
      "0         Non-trainable params\n",
      "101 K     Total params\n",
      "0.407     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d9337179bb433c9f9bc7130a021de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# init model\n",
    "autoencoder = LitAutoEncoder()\n",
    "\n",
    "# most basic trainer, uses good defaults (auto-tensorboard, checkpoints, logs, and more)\n",
    "# trainer = pl.Trainer(gpus=8) (if you have GPUs)\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "logger = TensorBoardLogger('tb_logs', name='my_model')\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=4, gpus=1, logger=logger, auto_scale_batch_size=True,\n",
    "                     default_root_dir=\"~/Shell_DS/91_lightning-hydra/lightning_logs\")\n",
    "\n",
    "# # train using Sharded DDP\n",
    "# from pytorch_lightning.plugins.ddp_sequential_plugin import DDPSequentialPlugin\n",
    "# trainer = Trainer(gpus=8, accelerator='ddp', plugins='ddp_sharded')\n",
    "\n",
    "# plugin = DDPSequentialPlugin(balance=[1, 1, 1, 1])\n",
    "# trainer = Trainer(accelerator='ddp', gpus=4, plugins=[plugin])\n",
    "\n",
    "# # find the batch size\n",
    "# trainer.tune(autoencoder)\n",
    "\n",
    "# tuner = Tuner(trainer)\n",
    "# # Invoke method\n",
    "# new_batch_size = tuner.scale_batch_size(model, *extra_parameters_here)\n",
    "\n",
    "# # Override old batch size (this is done automatically)\n",
    "# model.hparams.batch_size = new_batch_size\n",
    "\n",
    "\n",
    "trainer.fit(autoencoder, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ee4d93",
   "metadata": {},
   "source": [
    "The Trainer automates:\n",
    "\n",
    "- Epoch and batch iteration\n",
    "- Calling of optimizer.step(), backward, zero_grad()\n",
    "- Calling of .eval(), enabling/disabling grads\n",
    "- weights loading\n",
    "- Tensorboard (see loggers options)\n",
    "- Multi-GPU support\n",
    "- TPU\n",
    "- 16-bit precision AMP support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c4ae61",
   "metadata": {},
   "source": [
    "### LOGGERS\n",
    "Lightning supports the most popular logging frameworks (TensorBoard, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bd15dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def training_step(self, batch, batch_idx):\n",
    "#     self.log('my_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "#     img = ...\n",
    "#     log_image(img, self.trainer.log_dir)\n",
    "\n",
    "\n",
    "# class MyModule(LightningModule):\n",
    "#     def any_lightning_module_function_or_hook(self):\n",
    "#         some_img = fake_image()\n",
    "#         self.logger.experiment.add_image('generated_images', some_img, 0)\n",
    "\n",
    "\n",
    "# from pytorch_lightning.loggers import MLFlowLogger\n",
    "# mlf_logger = MLFlowLogger(experiment_name=\"default\", tracking_uri=\"file:./ml-runs\")\n",
    "# trainer = Trainer(logger=mlf_logger)\n",
    "\n",
    "\n",
    "# from pytorch_lightning.loggers import TensorBoardLogger\n",
    "# logger = TensorBoardLogger('tb_logs', name='my_model')\n",
    "\n",
    "\n",
    "# trainer = pl.Trainer(max_epochs=4, gpus=1, logger=logger, auto_scale_batch_size=True,\n",
    "#                      default_root_dir=\"~/Shell_DS/91_lightning-hydra/lightning_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e961976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-18 13:24:18.040667: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.4.1 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=\"tb_logs\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40fcf34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-18 13:30:10.950855: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.4.1 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=\"lightning_logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d331a4e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
